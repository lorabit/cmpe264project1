\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{epstopdf}
\usepackage{multirow}
\usepackage{graphicx}  %ziqiang
\usepackage{pythonhighlight}   %ziqiang

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{CMPE 264 - Project Assignment 1}

\author{Yanan Xie\\
{\tt\small yaxie@ucsc.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Ziqiang Wang\\
{\tt\small zwang232@ucsc.edu}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT

%%%%%%%%% BODY TEXT
\section{Camera radiometric calibration}
\label{sec:crc}

In this section, we describe how we take pictures and estimate the relationship between $B$ and $T$.

%-------------------------------------------------------------------------
\subsection{Taking pictures}

In order to simulate a surface with uniform radiance, we put a scratch paper on the table. Then, we took 9 pictures with a Canon 5D Mark II(DSLR) and a 40mm lens. A tripod was utilized to stabilize the camera. ISO was set to 100. F-number of the lens was set to 4. The exposure times of those 9 pictures are $1/25s$, $1/30s$, $1/40s$, $1/50s$, $1/60s$, $1/80s$, $1/100s$, $1/125s$, and $1/160s$ respectively. Pictures are all JPEG format. Figure \ref{fig:samplepicture} shows a picture taken with $T = 1/30s$ as an example. \\

Sample area was chosen to be the center of the picture with a size $100\times 100$. As shown in figure \ref{fig:sampler}, \ref{fig:sampleg} and \ref{fig:sampleb}, the sampled areas are not saturated in any channels. And the strips confirmed that the pictures were taken very stably.

\begin{figure}[bhp]
\includegraphics[width=\columnwidth]{images/_MG_6277}
\caption{Picture took with $T = 1/30s$}

\label{fig:samplepicture}
\end{figure}


\begin{figure}[t]
\centering
\subfigure[$T=1/160s$]{
\includegraphics[width=.3\columnwidth]{images/samples/0_2}
}
\subfigure[$T=1/125s$]{
\includegraphics[width=.3\columnwidth]{images/samples/1_2}
}
\subfigure[$T=1/100s$]{
\includegraphics[width=.3\columnwidth]{images/samples/2_2}
}
\subfigure[$T=1/80s$]{
\includegraphics[width=.3\columnwidth]{images/samples/3_2}
}
\subfigure[$T=1/60s$]{
\includegraphics[width=.3\columnwidth]{images/samples/4_2}
}
\subfigure[$T=1/50s$]{
\includegraphics[width=.3\columnwidth]{images/samples/5_2}
}
\subfigure[$T=1/40s$]{
\includegraphics[width=.3\columnwidth]{images/samples/6_2}
}
\subfigure[$T=1/30s$]{
\includegraphics[width=.3\columnwidth]{images/samples/7_2}
}
\subfigure[$T=1/25s$]{
\includegraphics[width=.3\columnwidth]{images/samples/8_2}
}

\caption{Sample areas of Channel R}
\label{fig:sampler}
\end{figure}

\begin{figure}[t]
\centering
\subfigure[$T=1/160s$]{
\includegraphics[width=.3\columnwidth]{images/samples/0_1}
}
\subfigure[$T=1/125s$]{
\includegraphics[width=.3\columnwidth]{images/samples/1_1}
}
\subfigure[$T=1/100s$]{
\includegraphics[width=.3\columnwidth]{images/samples/2_1}
}
\subfigure[$T=1/80s$]{
\includegraphics[width=.3\columnwidth]{images/samples/3_1}
}
\subfigure[$T=1/60s$]{
\includegraphics[width=.3\columnwidth]{images/samples/4_1}
}
\subfigure[$T=1/50s$]{
\includegraphics[width=.3\columnwidth]{images/samples/5_1}
}
\subfigure[$T=1/40s$]{
\includegraphics[width=.3\columnwidth]{images/samples/6_1}
}
\subfigure[$T=1/30s$]{
\includegraphics[width=.3\columnwidth]{images/samples/7_1}
}
\subfigure[$T=1/25s$]{
\includegraphics[width=.3\columnwidth]{images/samples/8_1}
}

\caption{Sample areas of Channel G}
\label{fig:sampleg}
\end{figure}

\begin{figure}[t]
\centering
\subfigure[$T=1/160s$]{
\includegraphics[width=.3\columnwidth]{images/samples/0_0}
}
\subfigure[$T=1/125s$]{
\includegraphics[width=.3\columnwidth]{images/samples/1_0}
}
\subfigure[$T=1/100s$]{
\includegraphics[width=.3\columnwidth]{images/samples/2_0}
}
\subfigure[$T=1/80s$]{
\includegraphics[width=.3\columnwidth]{images/samples/3_0}
}
\subfigure[$T=1/60s$]{
\includegraphics[width=.3\columnwidth]{images/samples/4_0}
}
\subfigure[$T=1/50s$]{
\includegraphics[width=.3\columnwidth]{images/samples/5_0}
}
\subfigure[$T=1/40s$]{
\includegraphics[width=.3\columnwidth]{images/samples/6_0}
}
\subfigure[$T=1/30s$]{
\includegraphics[width=.3\columnwidth]{images/samples/7_0}
}
\subfigure[$T=1/25s$]{
\includegraphics[width=.3\columnwidth]{images/samples/8_0}
}

\caption{Sample areas of Channel B}
\label{fig:sampleb}
\end{figure}

\subsection{Discovering the relation between $B$ and $T$}

$B$ is estimated by the average value of the sampled area $B'$. We calculate $B'$ in RGB channels independently in order to solve the non-linear function between $B'$ and $B$. \\

We assume $B(T) = (B'(T))^g = cETG = KT  $ following the assumption given by the instruction where $K = cEG$. By applying logarithm to both sides of the equation, we have 
$$ g\ln(B') = \ln(K) + \ln(T) $$
$$ \ln(B') = \ln(K)/g + \ln(T)/g = a + b\ln(T) $$ where $a = \ln(K)/g$ and $b=1/g$. \\

We used sklearn to estimate the coefficient of linear regression by taking $[\ln(T_i)]$ as the $X$ vector, and $[\ln(B'_i)]$ as the $Y$ vector. As shown in Table \ref{tab:linear}, the result suggests us to choose $g_R = 2.308$, $g_G = 1.962$ and $g_B = 1.484$ for RGB channels respectively. We show the $B'^g(T)$ curves before and after applied estimated $g$ in Figure \ref{fig:samplecurves}\\


\begin{table}[]
\caption{Linear regression results}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
Channel &  $a$  &  $b$ & $g$ & mean squared error  \\
\hline

Red & 7.011 & 0.4334 & 2.308 & 0.004156 \\
\hline
Green & 7.129& 0.5097 & 1.962 & 0.002331\\
\hline
Blue & 7.297 & 0.6737 & 1.484 & 0.0008915\\
\hline

 \hline

\end{tabular}
\label{tab:linear}
\end{table}


With estimation of $g$, and given the value of $B'(T) = (KT)^{1/g}$ for some pixel, we can estimate $B'(T_{dest})$ by 
\begin{equation}
\begin{aligned}
B'(T_{dest})     &=  (KT_{dest})^{1/g}\\
            &=  (KT)^{1/g}T_{dest}^{1/g}/T^{1/g}\\
            &=  B'(T) (\frac{T_{dest}}{T})^{1/g}
\end{aligned}
\label{equ:nonlinear}
\end{equation}
for the same pixel. Also, we can estimate $B(T)$ by
\begin{equation}
\begin{aligned}
B(T)     &=  B'^g(T)
\end{aligned}.
\label{equ:linear}
\end{equation}

\begin{figure}[t]
\centering
\subfigure[Red Channel $g=1$]{
\includegraphics[width=.47\columnwidth]{images/rchannel}
}
\subfigure[Red Channel $g=2.308$]{
\includegraphics[width=.47\columnwidth]{images/rchannelg}
}
\subfigure[Green Channel $g=1$]{
\includegraphics[width=.47\columnwidth]{images/gchannel}
}
\subfigure[Green Channel $g=1.962$]{
\includegraphics[width=.47\columnwidth]{images/gchannelg}
}

\subfigure[Blue Channel $g=1$]{
\includegraphics[width=.47\columnwidth]{images/bchannel}
}
\subfigure[Blue Channel $g=1.484$]{
\includegraphics[width=.47\columnwidth]{images/bchannelg}
}

\caption{$B'^g(T)$ Curves}
\label{fig:samplecurves}
\end{figure}

\section{Acquire a picture stack}
\label{sec:stack}

We took another 10 pictures with the same camera and a 17mm lens. ISO was set to 400 and F-number was set to 4. The optimal exposure time was chosen to be $T_0 = 1/100s$ so that there were no saturation(ensured by the warning from camera). Then we picked another two pictures with $T_1 = 1/4s$, $T_2 = 6s$, since they got good exposures on different layers as shown in Figure \ref{fig:picturestack}. \\

Equation \ref{equ:linear} was applied to obtain the linearized pictures. To ensure the values of $B(T)$ fall in the range $[0,255]$, we divided the values of $B(T) = B'^g(T)$ by $255^{g-1}$ for each channel. The result is also shown in Figure \ref{fig:picturestack}.

\begin{figure}[t]
\centering
\subfigure[Picture with $T=1/100s$]{
\includegraphics[width=.47\columnwidth]{images/hdr/_MG_6295}
}
\subfigure[Linearized (a)]{
\includegraphics[width=.47\columnwidth]{images/hdr/simple_linear/_MG_6295}
}

\subfigure[Picture with $T=1/4s$]{
\includegraphics[width=.47\columnwidth]{images/hdr/_MG_6298}
}
\subfigure[Linearized (c)]{
\includegraphics[width=.47\columnwidth]{images/hdr/simple_linear/_MG_6298}
}

\subfigure[Picture with $T=6s$]{
\includegraphics[width=.47\columnwidth]{images/hdr/_MG_6301}
}
\subfigure[Linearized (e)]{
\includegraphics[width=.47\columnwidth]{images/hdr/simple_linear/_MG_6301}
}

\caption{Picture Stack}
\label{fig:picturestack}
\end{figure} 

% QQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQ

\section{Create composite image}
\label{sec:create}
After linearization, we acquired three 3-dimensional matrices. We put them in a list imgs[] which will be used to create the composite HDR image.The imgs[0], imgs[1] and imgs[2] represent the linearized image at exposure time $T$, $a_{1}T$ and $a_{2}T$ respectively. Three different methords are applied to create the HDR image.
\begin{python}
imgs = []
for filename in valid_files:
	imgs.append(simple_linear(filename))
\end{python}

\subsection{Composition Algorithm 1}
For each pixel, use the pixel value of the image with largest exposure time such that the pixel is not saturated. \\
\begin{python}
for i in  range(len(imgs[0])):
	for j in range(len(imgs[0][0])):
		for k in range(len(imgs[0][0][0])):
			if imgs[2][i][j][k] < threshold:
				HDR_img_method_1[i][j][k] 
				= imgs[2][i][j][k] * 
				(exposure_times[0]/exposure_times[2])
			elif imgs[1][i][j][k] <threshold:
				HDR_img_method_1[i][j][k] 
				= imgs[1][i][j][k] * 
				(exposure_times[0]/exposure_times[1])
			else:
				HDR_img_method_1[i][j][k] 
				= imgs[0][i][j][k]
\end{python}

As showed in the Python code above, for each pixel and and each channel, If the value of exposure time $a_{2}T$ and $a_{1}T$ are both saturated, we will take the value of exposure time $T$, otherwise, we will use the value of lager exposure time. So this algorithm meets the requirement of Algorithm 1. The output of Algorithm 1 is shown in Figure \ref{fig:HDR_img_method_1}. 

\begin{figure}[bhp]
\includegraphics[width=\columnwidth]{images/hdr/combined/HDR_img_method_1}
\caption{Composite image with Algorithm 1}

\label{fig:HDR_img_method_1}
\end{figure}

\subsection{Composition Algorithm 2}
For each pixel, use the average of the values in the images for which this pixel is not saturated. \\
\begin{python}
for i in  range(len(imgs[0])):
	for j in range(len(imgs[0][0])):
		for k in range(len(imgs[0][0][0])):
			s = imgs[0][i][j][k] 
			n = 1
			if imgs[2][i][j][k] < threshold:
				n = n +1
				s = s + imgs[2][i][j][k] 
				* (exposure_times[0]/exposure_times[2])
			if imgs[1][i][j][k] <threshold:
				n = n + 1
				s = s + imgs[1][i][j][k] 
				* (exposure_times[0]/exposure_times[1])
			HDR_img_method_2[i][j][k] = s/n
\end{python}

Similarly to Algorithm 1, this algorithm will count all the values which are not saturated for each pixel and each channel, then divided by how many values we counted, so we have the average of the unsaturated values. The output of Algorithm 2 is shown in Figure \ref{fig:HDR_img_method_2}.

\begin{figure}[bhp]
\includegraphics[width=\columnwidth]{images/hdr/combined/HDR_img_method_2}
\caption{Composite image with Algorithm 2}

\label{fig:HDR_img_method_2}
\end{figure}

\subsection{Composition Algorithm 3}
Compute a weighted average of the unsaturated values. \\
\begin{python} 
a1 = exposure_times[1]/exposure_times[0]
a2 = exposure_times[2]/exposure_times[0]

weight1 = 1/(pow(a1,2))
weight2 = 1/(pow(a2,2))

for i in  range(len(imgs[0])):
	for j in range(len(imgs[0][0])):
		for k in range(len(imgs[0][0][0])):
			s = imgs[0][i][j][k] 
			n = 1
			if imgs[2][i][j][k] < threshold:
				n = n + weight2
				s = s + weight2 * imgs[2][i][j][k] 
				* (exposure_times[0]/exposure_times[2])
			if imgs[1][i][j][k] <threshold:
				n = n + weight1
				s = s + weight1 * imgs[1][i][j][k] 
				* (exposure_times[0]/exposure_times[1])
			HDR_img_method_3[i][j][k] = s/n
\end{python}

The weight for the pixel in the i-th image should be inversely proportional to the square of variance of the noise at that pixel for the image, which is propotional to exposure time. The output of Algorithm 2 is shown in Figure \ref{fig:HDR_img_method_3}.

\begin{figure}[bhp]
\includegraphics[width=\columnwidth]{images/hdr/combined/HDR_img_method_3}
\caption{Composite image with Algorithm 3}

\label{fig:HDR_img_method_3}
\end{figure}



\section{Reproduce composite image}
\label{sec:reproduce}
In this section we will reproduce the composite image, it will also help us have better understanding of the difference between the three HDR algorithms in section 3. \\

Now we have three composite HDR images by apply three algorithms. The value of each pixel is stored as float32 type, the reason we do this is in order to use the tone mapping function in opencv, which requires the input data type to be float32.
After tone map process, we convert the image to unsigned 8 bit and save the image. \\

When we use the tone map function, there is a gamma correction parameter $gamma$ need to be set, we read from the OpenCV API that the $gamma$ is positive value of type float. Generally speaking, if $gamma > 1$ the function will brighten the image, if $gamma < 1$ the function will darken the image. The $gamma$ value of $1.0$ implies no correction, $gamma$ equal to $2.2$ is suitable for most displays. We want to show how the three different algorithms work with the dark pixel, so we set $gamma = 2.2$ \\

After applying tone map to three HDR images composited by three algorithms accordingly, we acquired three after-tonemap images. We can tell from the after-tonemap images that how the threee HDR algorithms work differently on dark pixels. The Algorithm 1 uses the pixel value with largest exposure time such that the pixel is not saturated, we could consider this as the largest exposure time value which is unsaturated have all the weight. The Algorithm 2 use the average of the values in the images for which this pixel is not saturated, we could consider this as the unsaturated value have the same weight. As for the Algorithm 3, the weight of each unsaturated value is inversely proportional to the square of the exposure time as we showed before, so we could consider this as the larger exposure time a value has, the less weight this value takes. So the conclusion is that Algorithm 1 show more details of the dark pixels than Algorithm 2 and Algorithm 2 show more details of the dark pixels than Algorithm 3. However, Algorithm 3 has a better smooth transition between the dark and light than Algorithm 2 and Algorithm 2 has a better smooth transition than Algorthm 1. This is also shown in Figure \ref{fig:tonemap}.

\begin{figure}[t]
\centering
\subfigure[HDR Algorithm 1]{
\includegraphics[width=.47\columnwidth]{images/hdr/combined/HDR_img_method_1}
}
\subfigure[Algorithm 1 tone map]{
\includegraphics[width=.47\columnwidth]{images/hdr/tonemapped/res_tonemap1_8bit}
}

\subfigure[HDR Algorithm 2]{
\includegraphics[width=.47\columnwidth]{images/hdr/combined/HDR_img_method_2}
}
\subfigure[Algorithm 2 tone map]{
\includegraphics[width=.47\columnwidth]{images/hdr/tonemapped/res_tonemap2_8bit}
}

\subfigure[HDR Algorithm 3]{
\includegraphics[width=.47\columnwidth]{images/hdr/combined/HDR_img_method_3}
}
\subfigure[Algorithm 3 Tone map]{
\includegraphics[width=.47\columnwidth]{images/hdr/tonemapped/res_tonemap3_8bit}
}

\caption{Before and after tone map}
\label{fig:tonemap}
\end{figure} 


\section{Acknowledgment}
We did all the project together. However, Ziqiang Wang is mainly responsible for
Section \ref{sec:create} and Section \ref{sec:reproduce} while Yanan Xie is responsible for the rest parts of this project including providing his professional photography devices.\\

Thanks to \LaTeX\ for providing such a great document preparation system. And many appreciates to the authors of Python, numpy, sklearn, OpenCV and matplotlib.\\



\end{document}
